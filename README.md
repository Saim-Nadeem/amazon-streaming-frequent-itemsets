Amazon Metadata Streaming: Frequent Itemset Mining with Apriori, PCY & Anomaly Detection

[![Python](https://img.shields.io/badge/Built%20with-Python-blue.svg)](https://www.python.org/)
[![Apache Kafka](https://img.shields.io/badge/Stream-Apache%20Kafka-red.svg)](https://kafka.apache.org/)
[![MongoDB](https://img.shields.io/badge/Database-MongoDB-green.svg)](https://www.mongodb.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A real-time streaming pipeline to analyze **Amazon product metadata** using Kafka and Python. It features three concurrent consumers: **Apriori**, **PCY**, and **Anomaly Detection**, with results stored in **MongoDB**.

---

## ğŸ“ˆ Use Case

- Perform **market basket analysis** on large-scale product metadata.
- Stream data using Kafka for **real-time processing**.
- Detect **frequent itemsets**, generate **association rules**, and find **anomalies** in product patterns.

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Environment Setup

```bash
python -m venv env
source env/bin/activate  # On Windows: env\Scripts\activate
pip install -r requirements.txt
```

### 2ï¸âƒ£ Download NLTK Resources

```python
import nltk
nltk.download('punkt')
nltk.download('stopwords')
```

---

## ğŸ“¦ Requirements

- Python 3.7+
- Kafka + Zookeeper installed locally
- MongoDB running on port 27017

### ğŸ§° Dependencies (see requirements.txt)
- pandas, numpy, tqdm, swifter  
- nltk, bs4, streamlit  
- kafka-python, pymongo  
- scikit-learn

---

## ğŸ§¹ Data Preparation

1. Download Amazon metadata: https://cseweb.ucsd.edu/~jmcauley/datasets/amazon_v2/  
2. Sample down to 20GB: `All_Amazon_Meta_Sampled.json`
3. Preprocess:
  - Remove nulls
  - Clean HTML, punctuation
  - Tokenize and remove stopwords
4. Save cleaned output as line-separated JSON

---

## ğŸ” Kafka Architecture

### ğŸ¯ Producer
- Streams product transactions to Kafka topic (`amazon-meta`)

### ğŸ§  Consumer 1: Apriori
- Identifies frequent itemsets
- Generates association rules
- Stores to MongoDB:
  - `apriori`
  - `apriori_associationrules`

### ğŸ” Consumer 2: PCY
- Optimized version using hashed buckets
- Saves frequent pairs to:
  - `pcy_frequent_itemsets`
  - `pcy_association_rules`

### ğŸš¨ Consumer 3: Anomaly Detector
- Uses Z-scores to find statistical anomalies in product co-occurrences
- Stores anomalies to:
  - `anomalies`

---

## ğŸ—„ï¸ MongoDB Integration

**Database**: `Frequent_Itemset`

| Component   | Collection                | Description                         |
|-------------|----------------------------|-------------------------------------|
| Apriori     | `apriori`, `apriori_associationrules` | Frequent sets + rules     |
| PCY         | `pcy_frequent_itemsets`, `pcy_association_rules` | Hash-based rules |
| Anomalies   | `anomalies`                | Z-score anomaly logs                |

---

## â–¶ï¸ How to Run

1ï¸âƒ£ Ensure the following before you run:
- Kafka is installed in `~/kafka`
- MongoDB is running locally on port `27017`
- All Python files and `Bash_Rc.sh` are placed in `~/Documents`

2ï¸âƒ£ Give execution permissions to the script (first time only):

```bash
chmod +x Bash_Rc.sh
```

3ï¸âƒ£ Run the full pipeline with:

```bash
./Bash_Rc.sh
```

âœ… This will automatically:
- Start **Zookeeper** in a new terminal
- Start **Kafka Server**
- Start the **Producer**
- Start **Consumer 1** (Apriori)
- Start **Consumer 2** (PCY)
- Start **Consumer 3** (Anomaly Detection)

ğŸ–¥ï¸ Each component runs in its own terminal window using `gnome-terminal`
â„¹ï¸ Press Enter in each terminal after execution starts to close that window

---


This starts the producer and all 3 consumers.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ producer.py                   # Kafka producer
â”œâ”€â”€ apriori.py                    # Apriori consumer
â”œâ”€â”€ pcy.py                        # PCY consumer
â”œâ”€â”€ anomaly.py                    # Anomaly detection consumer
â”œâ”€â”€ Bash_Rc.sh                    # Kafka launch script
â”œâ”€â”€ Pre_Processing.ipnb           # Jupyter notebook for data prep
â”œâ”€â”€ requirements.txt              # Required libraries
â”œâ”€â”€ LICENSE                       # MIT license
â””â”€â”€ README.md                     # You're here!

```

---

## ğŸ“Œ Use Cases

- ğŸ›’ E-commerce recommender systems
- ğŸ“Š Real-time product analytics
- ğŸ§  Research on scalable data mining

---

## ğŸ“„ License

This project is licensed under the **MIT License**.  
See the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**Saim Nadeem**  
ğŸ”— GitHub: [Saim-Nadeem](https://github.com/Saim-Nadeem)

---

## ğŸ™Œ Acknowledgments

- UCSD Amazon Metadata Dataset  
- Apache Kafka & MongoDB Docs  
- Algorithm insights from research and GfG  
- nltk, tqdm, streamlit, and Python OSS libraries

---
